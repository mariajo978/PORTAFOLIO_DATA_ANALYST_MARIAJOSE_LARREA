{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> está compuesto por las siguientes variables referidas siempre al año 2018:\n",
    "\n",
    "1. **Month** 1-4\n",
    "2. **DayofMonth** 1-31\n",
    "3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "4. **FlightDate** fecha del vuelo\n",
    "5. **Origin** código IATA del aeropuerto de origen\n",
    "6. **OriginCity** ciudad donde está el aeropuerto de origen\n",
    "7. **Dest** código IATA del aeropuerto de destino\n",
    "8. **DestCity** ciudad donde está el aeropuerto de destino  \n",
    "9. **DepTime** hora real de salida (local, hhmm)\n",
    "10. **DepDelay** retraso a la salida, en minutos\n",
    "11. **ArrTime** hora real de llegada (local, hhmm)\n",
    "12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterrizó menos de 15 minutos más tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n",
    "13. **Cancelled** si el vuelo fue cancelado (1 = sí, 0 = no)\n",
    "14. **CancellationCode** razón de cancelación (A = aparato, B = tiempo atmosférico, C = NAS, D = seguridad)\n",
    "15. **Diverted** si el vuelo ha sido desviado (1 = sí, 0 = no)\n",
    "16. **ActualElapsedTime** tiempo real invertido en el vuelo\n",
    "17. **AirTime** en minutos\n",
    "18. **Distance** en millas\n",
    "19. **CarrierDelay** en minutos: El retraso del transportista está bajo el control del transportista aéreo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, daño de la aeronave, espera de la llegada de los pasajeros o la tripulación de conexión, equipaje, impacto de un pájaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulación (descanso del piloto o acompañante) , daños por mercancías peligrosas, inspección de ingeniería, abastecimiento de combustible, pasajeros discapacitados, tripulación retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegación de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no válido, retrasos de peso y equilibrio.\n",
    "20. **WeatherDelay** en minutos: causado por condiciones atmosféricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n",
    "21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorológicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tráfico aéreo, problemas con los controladores aéreos, etc.\n",
    "22. **SecurityDelay** en minutos: causado por la evacuación de una terminal, re-embarque de un avión debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n",
    "23. **LateAircraftDelay** en minutos: debido al propio retraso del avión al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora más tardía de la que estaba prevista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera línea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opción header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto no hace nada: la lectura es lazy así que no se lee en realidad hasta que ejecutemos una acción sobre flightsDF\n",
    "# Solamente se comprueba que exista el fichero en esa ruta, y se leen los nombres de columnas\n",
    "flightsDF = spark.read.option(\"header\", \"true\")\\\n",
    "                 .csv(\"gs://ucm__bucket/data/flights-jan-apr-2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el esquema (nombre y tipo de dato de cada columna). Esto son solamente metadatos, por lo que no es ninguna acción.\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las columnas son cadenas de caracteres porque no hemos indicado el tipo de dato para cada columna ni tampoco le hemos pedido a Spark que intente inferirlo a partir de los datos. No queremos que todas sean string porque hay algunas numéricas que deberían ser tratadas como tales. Vamos a intentar inferir el esquema. Esto supone una lectura un poco más lenta y también es más lento que una tercera opción que consiste en indicar explícitamente el esquema para los datos en el momento de la lectura, que es la opción recomendada si sabemos de antemano qué tipo va a tener cada columna. Si lo hiciésemos de esa manera, en caso de que no se pueda leer con ese esquema obtendríamos un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "flightsDF = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"gs://ucm__bucket/data/flights-jan-apr-2018.csv\") # pon aquí la ruta en tu bucket\n",
    "\n",
    "# Ensuciamos a propósito la variable ArrDelay para que pase a ser un string como suele pasar con frecuencia\n",
    "flightsDF = flightsDF.withColumn(\"ArrDelay\",\\\n",
    "                                 F.when(F.rand(123) < 0.1, \"NA\").otherwise(F.col(\"ArrDelay\")))\n",
    "\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tiene mejor pinta, aunque todavía hay algunas columnas cuyo tipo de dato sigue siendo string cuando la intuición nos dice que deberían ser enteros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas con Data Frames\n",
    "\n",
    "No todas las operaciones sobre un DataFrame de Spark requieren llevar a cabo cálculos en los executors. Algunas simplemente consultan *metadatos* del DataFrame. Puesto que los DataFrames siempre se manejan desde el driver, y toda la metainformación (como por ejemplo nombres y tipos de las columnas, número de particiones, etc) está en el driver, consultar dicha información no requiere calcular nada en los executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultamos las columnas que tiene el DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contamos el número de filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una de las primeras cosas que nos preguntamos sobre un dataset: número de filas y columnas (cuántos ejemplos y cuántas variables tenemos para describirlos). Puesto que vamos a llevar a cabo varias transformaciones al DataFrame `flightsDF` a partir de este punto, vamos a usar `cache`() para que Spark lo mantenga en memoria en lugar de liberar la memoria ocupada tras cada acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los nombres de columna. Esto son solo metadatos de DataFrame, y están en el driver. No es necesaria ninguna\n",
    "# operación sobre el cluster para recuperar la variable interna columns de cualquier DataFrame\n",
    "print(\"Los datos tienen {0} columnas\".format(len(flightsDF.columns)))\n",
    "\n",
    "flightsDF.cache()        # Esta línea no hace cálculos, pero Spark anota que debe mantener este DF en memoria tras la primera vez que sea materializado\n",
    "rows = flightsDF.count() # Esto es una acción que obligará a que flightsDF sea materializado. Para ello, habrá que llevar a cabo\n",
    "                         # las transformaciones que lo generan en la celda anterior: read y withColumn, que están pendientes\n",
    "\n",
    "print(\"Los datos tienen {0} filas\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.is_cached       # consultamos el flag booleano que hay en el driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasaría si no cacheamos flightsDF? Probemos a ejecutar las celdas anteriores sin cachear y cacheando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar columnas por nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = flightsDF.select(\"Month\", \"DayofMonth\", \"ArrTime\")\n",
    "resultDF.show(10) # los nombres son sensibles a mayúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtramos (retenemos) filas en base a los valores de una o varias columnas\n",
    "\n",
    "Esto se puede hacer con sintaxis de SQL puro, o bien con operaciones sobre objetos columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de las transformaciones están definidas en el paquete `pyspark.sql.functions`, por lo que es frecuente importar el paquete completo con un alias, como `F`, en lugar de importar cada función individual. A partir de ese momento usamos `F.` antes del nombre de cada función, para decirle a python dónde buscar esa función.\n",
    "\n",
    "Mostramos también cómo encadenar varias transformaciones, sean la misma o distintas transformaciones combinadas una tras otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsJanuary20 = None\n",
    "flightsJanuary20 = flightsDF\\\n",
    "                      .where(\"DayofMonth = 20 and Month = 1\")\\\n",
    "                      .withColumn(\"DistKm\", 1.6 * flightsDF[\"Distance\"])\\\n",
    "                      .withColumn(\"DobleDistancia\", 2 * F.col(\"DistKm\"))\n",
    "#                      .select(\"Month\", \"ArrTime\", \"DistKm\") # encadenamos dos transformaciones: esto no desencadena ninguna operación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsJanuary20.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función col se utiliza para decir que nos estamos refiriendo a la columna cuyo nombre se pasa como argumento\n",
    "flightsJanuary20 = flightsDF\\\n",
    "                      .where((F.col(\"DayofMonth\") == 20) & (F.col(\"Month\") == 1))\\\n",
    "                      .select(\"Month\", \"ArrTime\")\\\n",
    "                      .cache() # encadenamos dos transformaciones: esto no desencadena ninguna operación\n",
    "\n",
    "# Cúantos vuelos hay el 20 de enero de 2018\n",
    "# La operación count() es una acción, así que obliga a materializar flightsJanuary20. Para ello es necesario ejecutar\n",
    "# las transformaciones where() y select() que pusimos en esta celda, aplicadas a flightsDF. De hecho, si no hubiésemos\n",
    "# cacheado flightsDF en las celdas anteriores, también habría que materializarlo otra vez, y para eso se leería de \n",
    "# nuevo el CSV desde HDFS\n",
    "rowsJanuary20 = flightsJanuary20.count()\n",
    "\n",
    "print(\"Hubo {0} vuelos el 20 de enero de 2018\".format(rowsJanuary20))\n",
    "\n",
    "# Esto es otra acción aplicada sobre el DataFrame flightsJanuary. Como flightsJanuary NO ha sido cacheado,\n",
    "# entonces las operaciones \"where\" y \"select\" se necesitan ejecutar DE NUEVO para poder hacer el show()\n",
    "flightsJanuary20.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No olvidemos cachear el DataFrame cuando tengamos previsto hacer varias operaciones sobre él, o de lo contrario estaremos repitiendo muchas veces los cálculos previos que llevaron a ese DataFrame!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos indicar el filtrado como un string con un trozo de código SQL\n",
    "# Recordemos que where y filter son exactamente equivalentes\n",
    "flightsJanuary31 = flightsDF.filter(\"DayofMonth = 31 and Month = 1\") # transformación filter: Spark no ejecuta nada\n",
    "\n",
    "flightsJanuary31.count() # acción count: obliga a materializar flightsJanuary31 para lo cual se tiene que ejecutar filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>RECUERDA:</b> Spark hace todas estas operaciones de manera distribuida en el cluster, por tanto cada nodo del cluster está filtrando filas de entre aquellas que están presentes en ese nodo (más precisamente, en ese executor). Cada executor envía al driver el recuento de cuántas filas ha filtrado, y los resultados son agregados en el driver para mostrar el recuento total.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: muestra por pantalla el retraso en la llegada, el aeropuerto de origen y de destino de aquellos vuelos que tuvieron lugar en Domingo y con un retraso a la llegada mayor de 15 minutos. Muestra el esquema de dicho DataFrame resultante.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDomingoDF = flightsDF.filter(\"DayOfWeek = 7 and ArrDelay > 15\")\\\n",
    "                            .select(\"ArrDelay\", \"Origin\", \"Dest\")\n",
    "flightsDomingoDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No utilices `show` al final de una secuencia de transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>CUIDADO</b>: si pretendes almacenar el DF resultante de una secuencia de transformciones encadenadas, no pongas nunca `show()` al final de esa secuencia! La acción `show()` devuelve `None` (valor nulo de Python) y por tanto, lo que estarás almacenando en esa variable será el nulo y no podrás seguir utilizando la variable. Lo correcto es almacenar el resultado de la última transformación, y luego imprimir el DF.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsJanuary20 = flightsDF\\\n",
    "                      .where((F.col(\"DayofMonth\") == 20) & (F.col(\"Month\") == 1))\\\n",
    "                      .select(\"Month\", \"ArrTime\")\\\n",
    "                      .show()  # ¡¡¡ ERROR !!!\n",
    "\n",
    "flightsJanuary20.select(\"Month\") ### ERROR: flightsJanuary20 almacena None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existen distintas sintaxis que podemos usar para referirnos a objetos Column de un DF de Spark\n",
    "\n",
    "1. Podemos usar `F.col(\"nombreColumna\")`\n",
    "2. También `nombreDataFrame.nombreColumna`\n",
    "3. Y también `nombreDataFrame[\"nombreColumna\"]`\n",
    "\n",
    "Pero las dos últimas tienen un problema, como podemos ver abajo: sólo pueden utilizarse sobre un DF que tenga nombre, es decir, que esté almacenado en alguna variable. Pero no todos lo están...\n",
    "\n",
    "Por ejemplo: *si intentamos aplicar las opciones 2 y 3 sobre una columna que hemos creado justo en un **DataFrame intermedio**, no podremos*, porque cualquier DF intermedio en una secuencia de transformaciones *encadenadas* **no tiene nombre**, y por tanto es imposible referirnos a una columna como `nombreDF.nombreColumna` porque no tenemos ninguna manera de referirnos a ese DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no puedo poner flightsDF.DistKm porque DistKm no pertenece a flightsDF !!\n",
    "\n",
    "flightsDF.withColumn(\"DistKm\", 1.6*F.col(\"Dist\"))\\\n",
    "         .select(flightsDF.Origin, flightsDF.Dest, F.col(\"DistKm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, la sintaxis `F.col(\"nombreColumna\")` **funciona siempre**, ya que está indicando un objeto columna que realmente *toma contexto* en el momento en el que lo utilizamos dentro de alguna transformación (por ejemplo select, withColumn, ...). Si el DF sobre el cual estamos aplicando la transformación, aunque dicho DF no tenga nombre, sí posee la columna a la que nos referimos dentro de F.col, entonces la operación es correcta y se puede realizar sin problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_orig = F.col(\"origDistintos\")\n",
    "\n",
    "distinctOrigins = flightsDF.select(F.col(\"ArrDelay\"), flightsDF.Distance, flightsDF[\"Origin\"])\\\n",
    "                            .select(F.lit(F.min(\"ArrDelay\")).alias(\"minimo\"),\n",
    "                                    F.mean(\"Distance\").alias(\"distMedia\"),\n",
    "                                    F.countDistinct(\"Origin\").alias(\"origDistintos\"))\\\n",
    "                            .select(\"minimo\", F.col(\"distMedia\"), F.col(\"origDistintos\"))\\\n",
    "                            .withColumn(\"mitadOrigin\", columna_orig / 2)  # OJO: no se pueden mezclar columnas de una sola fila con columnas normales\n",
    "distinctOrigins.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear y seleccionar columnas al vuelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar `select` no solo para seleccionar columnas existentes sino también para crear nuevas columnas al vuelo y a la vez seleccionarlas. Vamos a seleccionar Origin y Dest, que ya existen, a la vez que seleccionamos una tercera columna que estamos creando al vuelo, con la transformación de la distancia a km. Le ponemos como nombre \"DistanceKm\" en ese momento que la estamos creando, mediante la función `alias`. Si no usamos `alias`, Spark le pondrá a la nueva columna un nombre por defecto que en este caso sería \"1.6 * Distance\". Se recomienda usar alias para dar nombre a las nuevas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es una posibilidad: selectExpr poniendo expresiones SQL puras\n",
    "flightsDF.selectExpr(\"Origin\", \"Dest\", \"1.6*Distance AS DistanceKm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: crear un DataFrame con tres columnas seleccionando \"Origin\", \"OriginCity\" y una nueva columna de tipo string creada concatenando Origin y OriginCity con un guión \"-\". Utilizar `withColumn` y dentro la función `concat_ws` (concatenar con separador) con sintaxis <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat_ws\">F.concat_ws(\"-\", columna1, columna2)</a> del paquete pyspark.sql.functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenadoDF = flightsDF.withColumn(\"concatenado\", F.concat_ws(\"-\", F.col(\"Origin\"), F.col(\"OriginCity\")))\\\n",
    "                         .select(F.col(\"Origin\"), F.col(\"OriginCity\"), F.col(\"concatenado\"))\n",
    "concatenadoDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuidado: no es posible seleccionar columnas de \"muchas filas\" y de \"una sola fila\" a la vez\n",
    "\n",
    "Cuando aplicamos una función de agregación a una columna completa (p.ej. el máximo de una columna), el resultado es otro objeto Column de *una sola fila*. No se puede seleccionar ese objeto a la vez que una columna que sí tiene múltiples filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es correcto porque todas las columnas seleccionadas o creadas tienen una sola fila\n",
    "minimosDF = flightsDF.select(F.max(\"ArrDelay\").alias(\"maxDelay\"),\n",
    "                             F.min(\"ArrDelay\").alias(\"minDelay\"))\n",
    "\n",
    "# Esto no es correcto porque la primera columna tiene muchas filas pero la columna maxDelay tiene una sola fila\n",
    "badDF = flightsDF.select(F.col(\"ArrDelay\"), # ERROR!!!\n",
    "                         F.max(\"ArrDelay\").alias(\"maxDelay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar filas únicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacerse mejor idea de cómo es una variable categórica, es lógico querer cuántos valores distintos existen en nuestro dataset. Si consideramos todas las columnas, sería raro tener dos filas exactamente iguales en todos los valores, pero si seleccionamos solamente una o unas pocas columnas, podemos ver cuántas combinaciones distintas de valores de esas columnas se dan en el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctFlights = flightsDF.distinct()  # distinct es una transformación que devuelve el DF sin las filas repetidas\n",
    "distinctFlightsCount = distinctFlights.count() # count es una acción y provoca que se ejecute la transformación distinct\n",
    "\n",
    "print(\"Hay {0} filas distintas\".format(distinctFlightsCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si seleccionamos solo las columnas `Origin` y `Dest` y nos quedamos con las filas distintas (quitamos repetidos), entonces obtenemos un DataFrame con los posibles aeropuertos de origen y destino, es decir, aquellos trayectos que existen en un vuelo (pueden aparecer varias veces porque seguramente existirán muchos vuelos a lo largo de 2018 entre un origen y un destino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>TU TURNO</b>: ¿<b>cuántas</b> combinaciones de Origin y Dest existen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta en la siguiente celda:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countOriginDests = flightsDF.select(\"Origin\", \"Dest\").distinct().count()\n",
    "\n",
    "print(f\"Hay {countOriginDests} combinaciones de un aeropuerto de origen y uno de destino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque tengamos 2.5 millones de filas, solo hay 5795 combinaciones distintas de un aeropuerto de origen y otro de destino.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>TU TURNO</b>: ¿<b>cuántos</b> aeropuertos de origen existen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctOrigins = flightsDF.select(\"Origin\")\\\n",
    "                           .distinct()\\\n",
    "                           .count()\n",
    "        \n",
    "flightsDF.select(F.countDistinct(\"Origin\").alias(\"origDistintos\")).show()\n",
    "\n",
    "print(f\"Hay {distinctOrigins} aeropuertos desde los que puede partir un vuelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: vamos a hacer esto para cada columna, en un bucle, para hacernos a la idea de cuántos valores hay. Esto solo tiene sentido en realidad para columnas categóricas y no para numéricas donde casi todos los valores serán distintos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columnName in flightsDF.columns:\n",
    "\n",
    "    distinctValues = flightsDF.select(columnName).distinct().count()\n",
    "    \n",
    "    # No olvidéis indentar este comando para indicar que está dentro del cuerpo del bucle\n",
    "    print(f\"Existen {distinctValues} valores distintos en la columna {columnName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otra opción: función que recibe un DF y devuelve otro DF que tiene una sola fila con el conteo de valores distintos de cada columna\n",
    "\n",
    "Utilizamos list-comprehension para crear una **lista de objetos Column, que todavía no están ligados a ningún DataFrame** hasta que no utilicemos esa lista en alguna transformación (por ejemplo select) y entonces esos objetos columna **toman contexto**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_conteos(sparkDF):\n",
    "    # creamos una lista de objetos columna sin contexto (no pertenecen a ningún DF)\n",
    "    colsRecuento = [F.countDistinct(c).alias(c) for c in sparkDF.columns]\n",
    "    # Y ahora lo utilizamos en select y entonces Spark entiende a qúe DF nos referimos\n",
    "    return sparkDF.select(colsRecuento)\n",
    "\n",
    "conteosDF = df_conteos(flightsDF)\n",
    "conteosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Y si quisiéramos mostrar el número de valores solamente de aquellas columnas que sean categóricas?\n",
    "\n",
    "La variable interna `dtypes` que tienen todos los DF nos da una lista de parejas de ('nombreColumna', 'tipoDato') con la que podemos filtrar las columnas de tipo string para aplicar la operación de la celda anterior solamente a esas columnas. Hacemos list comprehension y un poco de pattern-matching para extraer directamente los dos campos de cada pareja en el bucle, en lugar de extraer un solo objeto tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_conteos_categoricas(sparkDF):\n",
    "    tipos = sparkDF.dtypes\n",
    "    colsRecuento = [F.countDistinct(c).alias(c) for (c, tipo) in sparkDF.dtypes if tipo == \"string\"]\n",
    "    return sparkDF.select(colsRecuento)\n",
    "\n",
    "conteosCategoricasDF = df_conteos_categoricas(flightsDF)\n",
    "conteosCategoricasDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear una nueva columna o reemplazar una existente por el resultado de operar con columnas existentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las distancias en millas. Vamos a convertirlas a km multiplicando las millas por 1.61. De nuevo, cada nodo del cluster hará esta operación localmente con los datos que se encuentran en ese nodo. El DataFrame resultante estará repartido en los mismos nodos. **No hay movimiento de datos ya que no se necesita ninguna información que no esté presente en ese nodo** para efectuar la operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn is a transformation returing a new DataFrame with one extra column appended on the right\n",
    "flightsWithKm = flightsDF.withColumn(\"DistanceKm\", F.col(\"Distance\") * 1.61)\n",
    "\n",
    "flightsWithKm.printSchema()\n",
    "\n",
    "flightsWithKm.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El día de la semana es una variable entera. Utilizar un entero o una variable categórica es una decisión que depende de qué tipo de modelo vayamos a ajustar. ¿Tiene sentido considerar días de la semana \"más grandes\" o \"más pequeños\", es decir, algo que se incrementa conforme \"se incrementa\" el día de la semana de 1 a 7? Aquí vamos a **reemplazar** la columna `DayOfWeek` que ya existía, por una versión categórica como strings. Utilizamos `withColumn` pasándole como primer argumento el nombre de una columna que ya existe, lo cual indica que queremos reemplazarla, en el mismo lugar que ocupaba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flightsCategoricalDay = flightsDF.withColumn(\"DayOfWeek\", F.when(F.col(\"DayOfWeek\") == 1, \"Monday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 2, \"Tuesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 3, \"Wednesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 4, \"Thursday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 5, \"Friday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 6, \"Saturday\")\\\n",
    "                                                           .otherwise(\"Sunday\"))\n",
    "\n",
    "flightsCategoricalDay.printSchema() # the column is still in the same position but has now string type\n",
    "\n",
    "flightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>CONSEJO:</b> El proceso de crear nuevas variables o <i>features</i> a partir de otras existentes, incluso incorporar variables de fuentes de datos externas (datos públicos) y de limpiar o reemplazar variables tras normalizarla se denomina genéricamente <i>feature engineering</i> (ingeniería de variables). A veces tiene mucho que ver con conocimientos específicos del dominio, aunque también con trucos estadísticos para normalizar siguiend métodos bien conocidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `when` es muy común para re-categorizar una variable o para crear nuevas variables categóricas a partir de condiciones complejas acerca de lo que les ocurre a los valores de otras columnas en esa misma fila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: crea una variable categórica de tipo string con dos categorías indicando si el día de la semana es laborable o es fin de semana. Los valores deben ser \"laborable\" o \"finde\". Utiliza `withColumn` y `when`. Será laborable cuando DayOfWeek esté entre 1 y 5, y fin de semana cuando sea 6 o 7 (en resumen: \"en otro caso\" es fin de semana). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsFindeLaborable = flightsDF.withColumn(\"Laborable\", F.when(F.col(\"DayOfWeek\")<= 5, \"Laborable\")\\\n",
    "                                                           .otherwise(\"Finde\"))\n",
    "\n",
    "flightsFindeLaborable.printSchema()\n",
    "flightsFindeLaborable.select(\"DayOfWeek\", \"Laborable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otra posibilidad: utilizar .isin() aplicado al objeto columna para expresar la condición del when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsCategoricalDay = flightsCategoricalDay.withColumn(\"TypeOfDay\", \n",
    "                            F.when(~(F.col(\"DayOfWeek\").isin({'Saturday','Sunday'})), \"laborable\")\\\n",
    "                             .otherwise(\"finde\"))\n",
    "flightsCategoricalDay.show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findeDF = flightsDF.withColumn(\"finde\", F.when(F.col(\"DayOfWeek\") <= 5, \"laborable\")\\\n",
    "                                         .otherwise(\"finde\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para reemplazamientos usando un solo diccionario, que tenga {'categoríaOriginal': 'nuevoValor'}. Ese diccionario puede haberse leído de algún fichero o similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from itertools import chain\n",
    "\n",
    "equivalencias = {\n",
    "    'LAX': 'LosAngeles', \n",
    "    'SFO': 'SanFrancisco'\n",
    "}\n",
    "\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*equivalencias.items())]) # creamos una columna de tipo diccionario\n",
    "\n",
    "# Ahora utilizamos esa columna de diccionario, pidiéndole los elementos asociados a las claves que indica la columna Origin \n",
    "# (que son las categorías originales). Por tanto nos devolverá otra nueva columna con todos los valores asociados a dichas claves, fila a fila\n",
    "flightsDF.withColumn(\"OriginRecat\", mapping_expr.getItem(F.col(\"Origin\")))\\\n",
    "         .select(\"Origin\", \"OriginRecat\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se puede usar la sintaxis de operador punto y similares para referirse a un objeto columna, solo cuando el DF al que pertenece esa columna tiene nombre\n",
    "\n",
    "OJO: en el siguiente ejemplo, no puedo poner flightsDF.DistKm porque DistKm no pertenece a flightsDF sino que pertenece al DF devuelto por `withColumn(...)` y dicho DF no tiene ningún nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no puedo poner flightsDF.DistKm porque DistKm no pertenece a flightsDF !!\n",
    "\n",
    "flightsDF.withColumn(\"DistKm\", 1.6*F.col(\"Dist\"))\\\n",
    "         .select(flightsDF.Origin, flightsDF.Dest, F.col(\"DistKm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir el tipo de dato de una columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con frecuencia, Spark no infiere correctamente el tipo de cada columna. \n",
    "* A veces ocurre que una columna numérica no se reconozca adecuadamente porque los datos tienen \"NA\" (un string) que quería significar data faltante en el dataset original. \"NA\" no es un string especial para Spark, por lo que simplemente reconoce que la columna tiene tanto enteros como strings, y el tipo más general que infiere es string para esa columna. \n",
    "* Pero no es el caso aquí porque no se da esto, y por eso hemos ensuciado al principio los datos a propósito. Vamos a limpiarlos y a convertir la columna `ArrDelay` a DoubleType como debe ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naCount = flightsDF.where(\"ArrDelay = 'NA'\").count()\n",
    "\n",
    "# Esto es sintaxis SQL, pero también podríamos haberla llamado como .where(F.col(\"ArrDelay\") == \"NA\"). Ambas son equivalentes.\n",
    "\n",
    "print(\"Hay \", naCount, \"filas con NA en ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "flightsDF = flightsDF.where((F.col(\"ArrDelay\") != \"NA\"))\\\n",
    "                     .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(T.DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenación respecto a una columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una transformación que nos devuelve otro DataFrame ordenado según la(s) columna(s) indicada(s), sea de forma ascendente o descendente. El DataFrame original no se modifica (al igual que ocurre en cualquier transformación). Se puede ordenar en base a una columna numérica (lo más frecuente) o también categórica (los strings se ordenan alfabéticamente). Para ordenar sí es necesario enviar información de unos nodos a otros puesto que no sabemos si existen o no datos mayores o menos que el valor que tenemos en el nodo (o más precisamente, en el worker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos los vuelos según ArrDelay\n",
    "sortedDF = flightsDF.orderBy(\"ArrDelay\")  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n",
    "\n",
    "# Orden ascendentemente por aeropuerto de origen (\"Origin\") y deshago los empates por arr_delay descendentemente\n",
    "sortedDF = flightsDF.orderBy(F.col(\"Origin\"), F.col(\"ArrDelay\").desc())  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n",
    "\n",
    "sortedDescDF = flightsDF.orderBy(\"ArrDelay\", ascending = False)\n",
    "sortedDescDF.select(\"ArrDelay\", \"Origin\", \"Dest\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traemos una sola fila al driver como objeto Row y accedemos a los campos con el operador . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeraFila = sortedDescDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeraFila.ArrTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeraFila.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaRows = sortedDescDF.take(20)  # devuelve una lista de Python, de objetos Row\n",
    "distancias = [r.Distance for r in listaRows]  # usamos sintaxis de listas por comprensión\n",
    "distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaLAX = flightsDF.where(\"Origin = 'LAX'\").collect() # ¡¡¡¡ CUIDADO !!!!\n",
    "len(listaLAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaLAX[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a comprobar que al llevar al driver las 10 primeras filas, el DF estaba ordenado así que esas filas están ordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ordenadoDF.take(10) # esto devuelve una lista de python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de agregación sobre el DataFrame completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene implementaciones distribuidas y paralelas de funciones de agregación frecuentes como la media, min, max y desviación típica entre otras. Todas estas funciones reciben como argumento el objeto columna sobre el que la función debe aplicarse. Lo más habitual es aplicarlas para agregar por grupos, pero esto lo veremos en el segundo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a seleccionar vuelos con `ArrDelay` mayor de 15 minutos, y con ellos vamos a crear columnas con el min, max, media y desviación típica de la columna `ArrDelay`. Crearemos y seleccionaremos dichas columnas al vuelo, como vimos en la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we select those flights with at least 16 minutes of delay and then compute the aggregations\n",
    "agregacionesDF = flightsDF.where(F.col(\"ArrDelay\") > 15)\\\n",
    "                          .selectariance(\"ArrDelay\").alias(\"varArrDelay\")\n",
    "                            )\n",
    "\n",
    "agregacionesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una función de Spark que hace casi todo esto por nosotros, llamada `summary`. Lo hace para cada columna numérica que encuentre en el dataset. Es también una transformación que devuelve un nuevo DataFrame con las métricas de resumen, sin modificar el DataFrame original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesDF = flightsDF.summary().cache()\n",
    "summariesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summariesDF.count()) # vamos a contar el número de filas del DF resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión a Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general es complicado leer los resultados que muestra Spark. En ocasiones interesa convertirlos a un dataframe de Pandas, aunque esto implica traer todas las filas al driver, lo cual **debe hacerse con mucho cuidado y solo en casos en los que estemos seguros de que el DataFrame de Spark es pequeño y no desbordará la memoria del driver**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><b>CONSEJO</b>: el concepto de dataframe como tabla con filas y columnas existe en muchos lenguajes de programación (pandas de Python, dataframes de R, DataFrames de Spark). Sin embargo, Spark maneja DataFrames que están físicamente distribuidos en las memorias RAM de las máquinas del cluster, lo cual no tiene nada que ver con lo que hace la biblioteca Pandas o R que se ejecutan en una sola máquina. Convertir un DataFrame de Spark en un dataframe de Pandas implica llevar todas las filas al driver, lo cual podría resultar en una excepción Out-of-Memory si el contenido del DataFrame es más grande que la memoria RAM de la máquina en la que se está ejecutando el programa driver. En este caso y en la mayoría de casos, se suele utilizar para mostrar resúmenes o agregados ya calculados previamente, y que sabemos que ocupan poco, con lo que no existe riesgo.</p>\n",
    "\n",
    "<p>Esta operación también es muy frecuente cuando queremos representar gráficamente el contenido de un DataFrame de Spark. No existen funciones gráficas en Spark, por lo que tenemos que convertirlo en un dataframe de Pandas y utilizar las funciones gráficas de Python habituales (matplotlib, Seaborn o incluso la propia biblioteca Pandas) para mostrar lo que necesitemos.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesDF.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsPd = summariesDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otroDF = spark.createDataFrame(flightsPd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
